from sentence_transformers import SentenceTransformer
from qdrant_client import QdrantClient
from qdrant_client.http import models
from qdrant_client.http.models import Distance, VectorParams
from typing import List, Dict, Any
import uuid
from langchain_core.documents import Document
from langchain_community.embeddings import HuggingFaceBgeEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from qdrant_client.http.models import Filter, FieldCondition, MatchValue, PointsSelector

class BgeDenseSearch:
    def __init__(
        self,
        model_name: str = "BAAI/bge-m3",
        qdrant_url: str = "http://localhost:6333",
        collection_name: str = "dense_collection",
        vector_size: int = 1024,
        recreate: bool = False
    ):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–∏—Å—Ç–µ–º—ã —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞.

        :param model_name: –ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ (–Ω–∞–ø—Ä–∏–º–µ—Ä, BAAI/bge-m3)
        :param qdrant_url: URL Qdrant —Å–µ—Ä–≤–µ—Ä–∞
        :param collection_name: –ò–º—è –∫–æ–ª–ª–µ–∫—Ü–∏–∏ –≤ Qdrant
        :param vector_size: –†–∞–∑–º–µ—Ä –≤–µ–∫—Ç–æ—Ä–∞
        :param recreate: –ü–µ—Ä–µ—Å–æ–∑–¥–∞—Ç—å –∫–æ–ª–ª–µ–∫—Ü–∏—é –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ
        """
        self.model = HuggingFaceBgeEmbeddings(
            model_name=model_name,
            encode_kwargs={'normalize_embeddings': True},
            query_instruction="Represent this sentence for searching relevant passages:"
        )
        self.client = QdrantClient(url=qdrant_url)
        self.collection_name = collection_name
        self.vector_size = vector_size
        self.splitter =  RecursiveCharacterTextSplitter(
            chunk_size=200,           # –¥–ª–∏–Ω–∞ —á–∞–Ω–∫–∞
            chunk_overlap=20,         # –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ –º–µ–∂–¥—É —á–∞–Ω–∫–∞–º–∏
            length_function=len,      # –º–æ–∂–Ω–æ –∑–∞–º–µ–Ω–∏—Ç—å –Ω–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
            separators=["\n\n", "\n", ". ", " ", ""]
        )

        if recreate:
            self._create_collection()

    def _create_collection(self):
        """–°–æ–∑–¥–∞—ë—Ç –∫–æ–ª–ª–µ–∫—Ü–∏—é –≤ Qdrant."""
        self.client.recreate_collection(
            collection_name=self.collection_name,
            vectors_config=VectorParams(size=self.vector_size, distance=Distance.COSINE),
        )
        print(f"–ö–æ–ª–ª–µ–∫—Ü–∏—è '{self.collection_name}' —Å–æ–∑–¥–∞–Ω–∞ —Å —Ä–∞–∑–º–µ—Ä–æ–º –≤–µ–∫—Ç–æ—Ä–∞ {self.vector_size}.")

    def add_docs(self, docs: list[Document]):
        try:
            for i,doc in enumerate(docs):
                # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º embeddings –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–∞
                source = doc.metadata["source"]
                print(f"start embedding document {source}")

                doc_points = []
                chunks = self.splitter.split_text(doc.page_content)
                for y, chunk in enumerate(chunks):
                    doc_points.append(models.PointStruct(
                        id= str(uuid.uuid4()),
                        vector=self.model.embed_query(chunk),
                        payload={
                            "page_content": chunk,
                            "chunk_id": y,
                            **doc.metadata  # –≤—Å–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–æ–±–∞–≤–ª—è—é—Ç—Å—è —Å—é–¥–∞
                        }
                    ))

                # –í—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ—á–∫–∏
                self.client.upsert(
                    collection_name=self.collection_name,
                    points=doc_points
                )
                print(f"upsert document chunks {source} successfully")

        except Exception as e:
            print(f"{e} error")

    def get_all_hashes_in_qdrant(self) -> set[str]:
        """
        –ü–æ–ª—É—á–∞–µ—Ç –í–°–ï file_hash –∏–∑ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ Qdrant —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ø–∞–≥–∏–Ω–∞—Ü–∏–∏.
        """
        hashes: set[str] = set()
        offset = None
        seen_points = 0
        limit = 1000  # –†–∞–∑–º–µ—Ä —Å—Ç—Ä–∞–Ω–∏—Ü—ã (—Ä–∞–∑—É–º–Ω—ã–π –±–∞–ª–∞–Ω—Å –º–µ–∂–¥—É –∫–æ–ª-–≤–æ–º –∑–∞–ø—Ä–æ—Å–æ–≤ –∏ –ø–∞–º—è—Ç—å—é)

        print("üîÑ –ù–∞—á–∏–Ω–∞–µ–º –ø–æ–ª—É—á–µ–Ω–∏–µ –≤—Å–µ—Ö —Ö–µ—à–µ–π –∏–∑ Qdrant...")

        while True:
            # –ü–æ–ª—É—á–∞–µ–º –ø–æ—Ä—Ü–∏—é —Ç–æ—á–µ–∫
            response = self.client.scroll(
                collection_name=self.collection_name,
                scroll_filter=None,  # –í—Å–µ —Ç–æ—á–∫–∏
                with_payload=True,
                with_vectors=False,
                limit=limit,
                offset=offset,
                order_by=None  # –ù–µ –Ω—É–∂–Ω–∞ —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ ‚Äî scroll —Å–∞–º –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –ø–æ—Ä—è–¥–æ–∫
            )

            points, next_offset = response

            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ö–µ—à–∏
            batch_hashes = {
                point.payload.get("file_hash")
                for point in points
                if point.payload and point.payload.get("file_hash")
            }
            hashes.update({h for h in batch_hashes if h})  # –ò—Å–∫–ª—é—á–∞–µ–º None

            seen_points += len(points)
            print(f"  ‚Üí –ó–∞–≥—Ä—É–∂–µ–Ω–æ {seen_points} —Ç–æ—á–µ–∫... (—É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ö–µ—à–µ–π: {len(hashes)})")

            # –£—Å–ª–æ–≤–∏–µ –≤—ã—Ö–æ–¥–∞
            if next_offset is None:
                break

            offset = next_offset

        print(f"‚úÖ –ó–∞–≤–µ—Ä—à–µ–Ω–æ. –í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ {len(hashes)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö file_hash")
        return hashes

    def search(self, query: str, limit: int = 5) -> List[Dict]:
        """
        –í—ã–ø–æ–ª–Ω—è–µ—Ç –ø–æ–∏—Å–∫ –ø–æ –∑–∞–ø—Ä–æ—Å—É.

        :param query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
        :param limit: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        :return: –°–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å —Ç–µ–∫—Å—Ç–æ–º –∏ –æ—Ü–µ–Ω–∫–æ–π
        """
        query_vector = self.model.embed_query(query)
        results = self.client.search(
            collection_name=self.collection_name,
            query_vector=query_vector,
            limit=limit
        )

        return [
            {
                "id": hit.id,
                "score": hit.score,
                "text": hit.payload.get("text"),
                "payload": hit.payload
            }
            for hit in results
        ]

    def count(self) -> int:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –∫–æ–ª–ª–µ–∫—Ü–∏–∏."""
        count_response = self.client.count(collection_name=self.collection_name)
        return count_response.count