# Исследование моделей и инфраструктуры

## Сравнение моделей
|Модель|Качество ответов|Скорость работы|Стоимость владения и использования|Удобство и простота развёртывания|
|--|--|--|--|--|
|||||
|Open AI - GPT 4|Эталонное. Лидер по пониманию контекста, креативности и сложному reasoning|Зависит от нагрузки OpenAI. Есть задержка сети.| Дорого. Плата за токены| Не нужно развёртывать, нужен только API-ключ и готово (и proxy в RU)|
|Yandex GPT|Сильна в русском языке и локальном контексте. Уступает GPT-4 в сложных задачах|Высокая. Низкая задержка в СНГ| Дешевле GPT-4. Есть бесплатный лимит | Не нужно развёртывать, нужен только API-ключ и готово|
|Qwen/Qwen2.5 (локально)|Сильна в многозычности (отличный английский и китайский, хороший русский).| Очень высокая.  Легко запускается (версии до 7B) на одной игровой GPU (RTX 4070+). Нет сетевой задержки|Цена видеокарты|Средняя. Есть готовые инструменты - Ollama, но требуется знание техпроцесса|
|Llama 3 70B (локально)|Выдающееся (для локальных). Максимально приближено к GPT-4 среди открытых моделей|Требует мощного железа (несколько высококлассных GPU, например, 2xH100). Быстрая на инференсе, но требует огромных ресурсов|стоимость сервера|Сложное. Требует кластера GPU и глубоких технических знаний для развертывания|

Облачные решения идеальны для быстрого старта и проектов, где критичны стабильность и качество. 
Локальные модели — выбор для проектов с большим объемом запросов, строгими требованиями к конфиденциальности данных 

## Сравнение моделей эмбеддингов
|Embedding model|Размер модели|Размер вектора (d)|Особенности|
|--|--|--|--|
|bge-m3|~2.2GB|1024| - Модель от BAAI (Китай)- Есть в Ollama- Можно использовать для reranking- КРУТЫЕ ФИЧИ!:       * Поддерживает три режима: dense, sparse и multi-vector       * Поиск и embedding с инструкциями|
|text-embedding-3|- (только API)|3072 (large)1536 (small)|- Модель от OpenAI- Не Open Source (Требует ключ API)- Требуется оплата API- Большой вектор - высокая точность|
|multilingual-e5|~1.1GB|1024 (large)|- Модель от Microsoft- Специально обучена для поиска- Отличный мультиязычный поиск|
|paraphrase-multilingual|~563MB|512|- Модель от SBERT (Sentence-BERT)- Есть в Ollama- В 2-5 раз быстрее других моделей из списка- Может работать на CPU- Менее точная|
|LaBSE|~1.7GB|768|- Модель от Google- Особенно сильна в кросс-языковых задачах (например, поиск на русском среди английских документов)|

*Чем выше размер вектора (d) и вес модели (GB/MB), тем точнее классификация и поиск, но тем медленнее и требует больше памяти (RAM/GPU)

## Сравние векторных баз
|Технология|Особенности|
|FAISS| - Библиотека, а не хранилище, хранит вектора в памяти- Больше подходит для прототипирования и проверки гипотез - Самая быстрая библиотека для ANN|
|Qdrant| - Написан на Rust- Поддержка фильтрации- Быстрый (gRPC API)- Поддерживает гибридный поиск (Sparse + Dense, экспериментально)- Поддержка quantization, для экономии памяти|
|Elasticsearch(от v7.1 и выше )|Поддерживает гибридный поиск (только Enterprise)- Для не Enterprise алгоритм гибридного поиска нужно осуществлять на backend приложении- Векторный поиск медленнее, чем в Qdrant- Высокие требования к памяти при больших векторах|
|pgvector для PostgreSQL| Производительность ниже, чем у Qdrant- Гибкость меньше, чем у других специализированных программ
|Vespa|- Поддерживает гибридный поиск- Поддерживает A/B тестирование - Open Source- Позволяет загружать предобученные модели- Можно использовать модели ML для ранжирования результатов|
|ChromaDB| Встроенная поддержка популярных эмбеддеров. Заточена под RAG из коробки. Легковесная, работает локально. Open Source|

## Вариант 1 (малый, локальный):
    Локальная средняя LLM (например, Qwen/Qwen2.5-3B)
    GPU: 1x RTX 4090 (24GB) или RTX 3090 (24GB).
    CPU: 12+ ядер.
    RAM: 32+ GB.
    Диски: NV SSD (1 TB).
    Плюсы: Намного дешевле Варианта A, адекватное качество для многих задач.
    Минусы: Качество ответов может уступать облачным аналогам.

## Вариант 2 (большой, локальный):
    Локальная мощная LLM (Qwen/Qwen2.5-72B или Llama 3 70B, используя 4-битное квантование)
    GPU: 2x RTX 4090 (24GB) или 1x A100 (40/80GB). Объединение памяти нескольких GPU критично для больших моделей.
    CPU: 16+ ядер (AMD Ryzen 9 / Intel i9).
    RAM: 64+ GB DDR4/5.
    Диски: Быстрый NV SSD (1-2 TB) для хранения моделей, индексов и данных.
    Плюсы: Полный контроль, данные не уходят наружу, низкая стоимость запроса.
    Минусы: Очень высокая начальная стоимость, требует экспертизы для настройки.

## Вариант 3 (большой, облачный) 
    Облачная LLM YandexGPT
    Сервер: Мощный CPU-сервер без необходимости в топовом GPU.
    CPU: 8+ ядер.
    RAM: 32+ GB (для работы с векторной БД и эмбеддинг-моделью).
    Диски: Быстрый SSD.
    Плюсы: Низкие начальные затраты на железо, лучшее качество ответов.
    Минусы: Постоянные операционные расходы, зависимость от интернета и API-провайдера.

Для CPU больше ядер на умеренной частоте (2.5-3 GHz) лучше, чем меньше ядер на более высокой частоте (>4.0 GHz) т.к. его основная деятельность - обеспечить стабильную, непрерывную передачу данных в GPU
Конфигурацию можно уточнить, исследуя исторические данные обращений к базам знаний (например посмотреть количество и частоту запросов в логах или запросы к бд)

## Предлагаемое решение - Вариант 1
Большие системы вырастают из маленьких. Если нет крайней необходимости в больших мошностях и нет специалистов с опытом, лучше начать с минимально-рабочего варианта, и затем масштабировать его.

Embedding model - выбрана модель ```BAAI/bge-m3``` за свою универсальность - мультиязычная, вшитая поддежка dense и sparse поиска, мультимодальная и поддержка инструкций
LLM Model - ```Qwen/Qwen2.5-3B```, относительно лёгкая модель работает в реал-тайме даже на среднем компьютере, мультиязычная и имеет большое контекстное окно (более 32к токенов, что много для моделей такого размера) - хороший кандидат для тестового MVP для RAG бота. После тестирования и настройки логирования и системы оценки можно использовать модель с большим количеством параметром, например ```Qwen/Qwen2.5-7B``` или ```Qwen/Qwen2.5-14B`` или выше, в зависимости от требованяи к качетву выдачи
Векторное хранилище - ```Qdrant```, боллее производительная чем другие локальные БД, простая масштабируемость. Большое и гибкое API для фильтрации по метаданным. Поддерживает квантование